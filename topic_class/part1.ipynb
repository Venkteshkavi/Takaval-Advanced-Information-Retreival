{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Member 1 Name : Emmanuel Johnson<br>\n",
    "Team Member 1 UBID : egnanach<br>\n",
    "Team Member 1 PersonNumber : 50290792<br>\n",
    "Team Member 2 Name : Venktesh Kaviarasan<br>\n",
    "Team Member 2 UBID : venktesh<br>\n",
    "Team Member 2 PersonNumber : 50289400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for Crawling Articles from New York Times and Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import codecs\n",
    "import time\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for getting data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import jsonpickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages for Processing the crawled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer as ss\n",
    "from nltk.stem import WordNetLemmatizer as wn\n",
    "#from nltk.stem import LancasterStemmer as ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFile(fileName, content):\n",
    "    file = open(fileName,\"w\") \n",
    "    file.write(content)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveUrlIndex(query, result_urls, ctype):\n",
    "    fName = \"../urls/\"+query+\"-\"+ctype\n",
    "    with codecs.open(\"%s-links.csv\" % fName,\"wb\",encoding=\"utf-8\") as output:\n",
    "        columns = [\"URL\"]\n",
    "        writer = csv.DictWriter(output,fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        for url in result_urls:\n",
    "            writer.writerow({\"URL\":url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnConfig = {\n",
    "    'golf': 'g',\n",
    "    'baseball': 'bsb',\n",
    "    'basketball': 'bkb',\n",
    "    'football': 'fb'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccConfig = {\n",
    "    \"mlb\": \"baseball\",\n",
    "    \"nfl\": \"football\",\n",
    "    \"nba\": \"basketball\",\n",
    "    \"golf\": \"golf\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Times Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_APP_KEY = \"WTjnKw4UrXjAY7E7FrIGX9HKYj1lbbY8\"\n",
    "NYT_APP_SECRET = \"e28wPvKPBTf2GcAd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New York Times API search request URL\n",
    "def formUrl(query, fromDate, toDate, fl, page=0):\n",
    "    url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?begin_date=\"+fromDate+\"&end_date=\"+toDate+ \\\n",
    "\"&fl=\"+fl+\"&q=\"+query+\"&api-key=\"+NYT_APP_KEY+\"&page=\"+str(page)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNYTUrls(url):\n",
    "    page = 0\n",
    "    result_url = list()\n",
    "    while True:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            resp = json.loads(response.content)\n",
    "            print(\"#### \", len(resp[\"response\"][\"docs\"]), \" ####\")\n",
    "            if len(resp[\"response\"][\"docs\"]) == 0:\n",
    "                break\n",
    "            for d in resp[\"response\"][\"docs\"]:\n",
    "                if d[\"web_url\"] not in result_url:\n",
    "                    result_url.append(d[\"web_url\"])\n",
    "        page += 1\n",
    "        url = \"&\".join(url.split(\"&\")[:-1])+\"&page=\"+str(page)\n",
    "        print(page, url)\n",
    "        if ((page+1) % 10) == 0:\n",
    "            time.sleep(61)\n",
    "    return result_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nytMain(query, folder):\n",
    "    #query = \"golf\" or \"baseball\" or \"basketball\" or \"football\"\n",
    "    url = formUrl(query, \"20190101\", \"20190331\", \"web_url,document_type\")\n",
    "    \n",
    "    #Get all the related urls for the given query\n",
    "    result_urls = getNYTUrls(url)\n",
    "    print(\"Total New York Times URLs : \"+str(len(result_urls)))\n",
    "    #Save the resulting url in a csv file\n",
    "    saveUrlIndex(query, result_urls, 'nyt')\n",
    "    \n",
    "    #Scrape the content of the urls recieved using nyt api\n",
    "    counter = 0\n",
    "    for url in result_urls:\n",
    "        fileName = \"../\"+folder+\"/nyt/\"+query+\"/\"+str(counter)+\".txt\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            parser = BeautifulSoup(response.content, 'html.parser')\n",
    "            #Search for section tag with name attribute as articleBody\n",
    "            article = parser.find(\"section\", {\"name\":\"articleBody\"})\n",
    "            if article:\n",
    "                #Get all the p tag texts\n",
    "                paras = article.find_all(\"p\")\n",
    "                if len(paras) > 0:\n",
    "                    content = \"\"\n",
    "                    for p in paras:\n",
    "                        content += str(p.text.encode('utf-8').strip(), 'utf-8')\n",
    "                    saveFile(fileName, content)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Crawl Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.bellingcat.com/resources/2015/08/13/using-python-to-mine-common-crawl/\n",
    "def getCCUrls(domain, index_list):\n",
    "    record_list = []\n",
    "    for index in index_list:\n",
    "        print(\"Current Index : \" + index)\n",
    "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-\"+index+\"-index?url=\"+domain+\"&output=json\"\n",
    "        print(\"Current URL : \" + cc_url)\n",
    "        response = requests.get(cc_url)\n",
    "        if response.status_code == 200:\n",
    "            records = response.content.splitlines()\n",
    "            for record in records:\n",
    "                record_list.append(json.loads(record))\n",
    "    print(\"# Records Found : \" + str(len(record_list)))\n",
    "    return record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHtmlDoc(record):\n",
    "    offset, length = int(record['offset']), int(record['length'])\n",
    "    offset_end = offset + length - 1\n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "    resp = requests.get(prefix + record['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "    raw_data = BytesIO(resp.content)\n",
    "    f = gzip.GzipFile(fileobj=raw_data)\n",
    "    data = f.read()\n",
    "    response = \"\"\n",
    "    if len(data):\n",
    "        try:\n",
    "            warc, header, response = data.strip().decode(encoding='utf-8', errors='strict').split('\\r\\n\\r\\n', 2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccMain(query, folder, domains):\n",
    "    #mlb - baseball || nfl - football || nba - basketball || golf - golf\n",
    "    #query = \"golf\" or \"mlb\" or \"nfl\" or \"nba\"\n",
    "    \n",
    "    #Common crawl index list : 2019-04 for Jan, 2019-09 for Feb and 2019-13 for Mar (YYYY-Week)\n",
    "    index_list = [\"2019-04\",\"2019-09\",\"2019-13\"]\n",
    "    \n",
    "    #For the given domains, collect all records present for the given date range (index_list)\n",
    "    record_list = list()\n",
    "    for domain in domains:\n",
    "        record_list += getCCUrls(domain, index_list)\n",
    "    print(\"Total Common Crawl URLs : \", len(record_list))\n",
    "    \n",
    "    #Scrape the content using the url data from the records recieved from common crawl\n",
    "    result_urls = list()\n",
    "    temp = list()\n",
    "    counter = 0\n",
    "    for record in record_list:\n",
    "        fileName = \"../\"+folder+\"/cc/\"+ccConfig[query]+\"/\"+str(counter)+\".txt\"\n",
    "        url = urlparse(record['url'])\n",
    "        #Ignore http urls because it's a duplicate of a https url\n",
    "        if url.scheme == \"http\":\n",
    "            continue\n",
    "        urlString = url.geturl()\n",
    "        #Remove query strings from the url\n",
    "        strippedUrl = urlString[:urlString.find('?')]\n",
    "        urlPath = url.path\n",
    "        #Only scrape if the url is not scraped before (removes duplicates)\n",
    "        if urlPath not in temp:\n",
    "            result_urls.append(strippedUrl)\n",
    "            temp.append(urlPath)\n",
    "            html_content = getHtmlDoc(record)\n",
    "            parser = BeautifulSoup(html_content)\n",
    "            #Search for div tag with itemprop attribute as articleBody\n",
    "            article = parser.find(\"div\", {\"itemprop\":\"articleBody\"})\n",
    "            if article:\n",
    "                #Get the text from all the p tag with class name as p-text\n",
    "                paras = article.find_all(\"p\", {\"class\":\"p-text\"})\n",
    "                if len(paras) > 0:\n",
    "                    content = \"\"\n",
    "                    for p in paras:\n",
    "                        content += str(p.text.encode('utf-8').strip(), 'utf-8')\n",
    "                    saveFile(fileName, content)\n",
    "        counter += 1\n",
    "    print(len(result_urls))\n",
    "    saveUrlIndex(query, result_urls, 'cc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credentials_checker():\n",
    "    twitter_cred = dict()\n",
    "    twitter_cred['CONSUMER_KEY'] = \"GlQmi0BC7y1jspiPu3xWtcUon\"\n",
    "    twitter_cred['CONSUMER_SECRET'] = \"KAnScl27IGPom4nVYw44126ursS7c1TiKlfG6TRkGBFieeK0Sm\"\n",
    "    twitter_cred['ACCESS_KEY'] = \"1357145954-ZIsxSkVYa8ixkMVMwA0Wm6pFhoF4MYsM1whvU8c\"\n",
    "    twitter_cred['ACCESS_SECRET'] = \"QuJ6qR8bfThPuAjjHnyQz0gjknLd6o8BJW1Iqh7NO00M3\"\n",
    "\n",
    "    with open('twitter_credentials.json', 'w') as secret_info:\n",
    "        json.dump(twitter_cred, secret_info, indent=4, sort_keys=True)\n",
    "\n",
    "    with open(\"twitter_credentials.json\") as cred_data:\n",
    "        info = json.load(cred_data)\n",
    "        consumer_key = info['CONSUMER_KEY']\n",
    "        consumer_secret = info['CONSUMER_SECRET']\n",
    "        access_key = info['ACCESS_KEY']\n",
    "        access_secret = info['ACCESS_SECRET']\n",
    "    \n",
    "    #Create API Endpoint\n",
    "    auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "    if(not api):\n",
    "        print(\"Problem Connecting to API\")\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tweets(sport,search_query,api):\n",
    "    dict_store = \"tweet_dict\" + \"_\" + sport\n",
    "    dict_store = {}\n",
    "    query_term = sport + \"_query\"\n",
    "    query_term = search_query + \"-filter:retweets\"\n",
    "    max_tweets = 12000\n",
    "    tweet_count = 0\n",
    "\n",
    "    #Using Tweepy Cursor to fetch Tweets\n",
    "    print(\"Collecting Tweets\")\n",
    "    for tweet in tqdm(tweepy.Cursor(api.search, q=query_term, geocode=\"39.7837304,-100.4458825,2000mi\",lang='en').items(max_tweets)):\n",
    "        if tweet.id in dict_store.keys():\n",
    "            continue\n",
    "        else:\n",
    "            dict_store[tweet.id] = tweet.text\n",
    "            tweet_count += 1\n",
    "    print(\"Downloaded {0} tweets\".format(tweet_count))\n",
    "    return dict_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_writer(sport,myDict):\n",
    "    print(\"Writing the tweets into a file\")\n",
    "    with open(\"../new_data/tw/tw_\"+fnConfig[sport]+\".csv\", \"w\") as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"tweet_Id\",\"tweet_text\"])\n",
    "        for key, value in myDict.items():\n",
    "            writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetMain(sport):\n",
    "    print(\"Checking Credentials & Creating end point\")\n",
    "    API=credentials_checker()\n",
    "    \n",
    "    #sport = input(\"Specify the Sport Name to Collect Tweets   \")\n",
    "    query_generator = sport + \"_query\"\n",
    "    dict_generator = sport + \"_dict\"\n",
    "    \n",
    "    print(\"Verifying Input......\")\n",
    "    if(sport == \"basketball\"):\n",
    "        query_generator = ('NBA OR \"NCAA MBB\" OR NCAA OR OR Basketball OR \"NCAA WBB\" OR #NBAPlayoffs OR #NCAA OR #USBasketball OR #BasketballYouth')\n",
    "        dict_generator = find_tweets(sport,query_generator,API)\n",
    "        tweet_writer(sport,dict_generator)\n",
    "    \n",
    "    elif(sport == \"football\"):\n",
    "        query_generator = ('\"USA Football\" OR NFL OR #USAFootball OR #NFL OR #NFLDraftNews OR #Quarterbacks OR #AmericanFootball')\n",
    "        dict_generator = find_tweets(sport,query_generator,API)\n",
    "        tweet_writer(sport,dict_generator)\n",
    "    \n",
    "    elif(sport == \"golf\"):\n",
    "        query_generator = ('LPGA OR \"Golf USA\" OR \"Masters Tournament\" OR \"USC Men Golf\" OR #golfUSA OR #PGATour OR #golfnews OR #ANWAgolf OR #golf OR #TeamTitleist OR #golfchannel OR #AggieGolf OR #golfweek')\n",
    "        dict_generator = find_tweets(sport,query_generator,API)\n",
    "        tweet_writer(sport,dict_generator)\n",
    "\n",
    "    elif(sport == \"baseball\"):\n",
    "        query_generator = ('MLB OR \"Minor League Baseball\" OR baseball OR #BlueJays OR #Padres OR #Rockies OR #Mariners OR #GeauxTigers OR #IUBase OR #HailState OR #Pirates OR #ClawsUp')\n",
    "        dict_generator = find_tweets(sport,query_generator,API)\n",
    "        tweet_writer(sport,dict_generator)\n",
    "    \n",
    "    else:\n",
    "        print(\"Wrong Search Term Cannot Process Request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemSentence(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    final = \"\"\n",
    "    sb = ss(\"english\")\n",
    "#     wnl = wn()\n",
    "    for w in words: \n",
    "        root = sb.stem(w)\n",
    "#         root = wnl.lemmatize(w)\n",
    "        final += root\n",
    "        final += \" \"\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processArticles(files, dest, fn):\n",
    "    counter = 0\n",
    "    for f in files:\n",
    "        sample = open(f,\"r\")\n",
    "        refinedFinal = \"\"\n",
    "        print(\"Processing File # {}\".format(counter), end=\"\\r\", flush=True)\n",
    "        for line in sample.readlines():\n",
    "            lowerLine = line.lower()\n",
    "            noUrls = re.sub(r\"http\\S+\", \"\", lowerLine)\n",
    "            noUnderscore = noUrls.replace(\"_\", \"\")\n",
    "            #remove digits\n",
    "            noNoLine = re.sub(r'\\d+', '', noUnderscore)\n",
    "            #remove punctuations\n",
    "            words = re.findall(r'\\w+', noNoLine, flags = re.UNICODE)# | re.LOCALE\n",
    "            #remove stop words\n",
    "            important_words = filter(lambda x: x not in stopwords.words('english') and x.isdigit() == False and x not in letters, words)\n",
    "            refined = \" \".join(important_words)\n",
    "            #Get root words for the given words\n",
    "            refinedFinal += stemSentence(refined)\n",
    "            refinedFinal += \"\\n\"\n",
    "        saveFile(dest+fn+str(counter)+\".txt\", refinedFinal)\n",
    "        counter += 1\n",
    "        sample.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweets(csvFile, dest):\n",
    "    fn = csvFile.split(\".csv\")[0].split(\"/\")[-1]\n",
    "    df = pd.read_csv(csvFile)\n",
    "    tweets = df.tweet_text\n",
    "    refinedFinal = \"\"\n",
    "    count = 0\n",
    "    tempList = list()\n",
    "    for tweet in tweets:\n",
    "        print(\"Processing Tweet # {}\".format(count), end=\"\\r\", flush=True)\n",
    "        lowerLine = tweet.lower()\n",
    "        noUrls = re.sub(r\"http\\S+\", \"\", lowerLine)\n",
    "        noUnderscore = noUrls.replace(\"_\", \"\")\n",
    "        #remove digits\n",
    "        noNoLine = re.sub(r'\\d+', '', noUnderscore)\n",
    "        #remove punctuations\n",
    "        words = re.findall(r'\\w+', noNoLine, flags = re.UNICODE)# | re.LOCALE\n",
    "        #remove stop words\n",
    "        important_words = filter(lambda x: x not in stopwords.words('english') and x.isdigit() == False and x not in letters, words)\n",
    "        refined = \" \".join(important_words)\n",
    "        #Get root words for the given words\n",
    "        refinedStemmed = stemSentence(refined)\n",
    "        if refinedStemmed in tempList:\n",
    "            continue\n",
    "        tempList.append(refinedStemmed)\n",
    "        refinedFinal += refinedStemmed\n",
    "        refinedFinal += \"\\n\"\n",
    "        count += 1\n",
    "    saveFile(dest+fn+\".txt\", refinedFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataMain(directory, dest, ctype):\n",
    "    if ctype == \"twitter\":\n",
    "        processTweets(directory, dest)\n",
    "    else:\n",
    "        files=glob.glob(directory+\"*.txt\")\n",
    "        fn = ctype+\"_\"+fnConfig[directory.split(\"/\")[-2]]+\"_\"\n",
    "        processArticles(files, dest, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New York Times<br>\n",
    "Common Crawl<br>\n",
    "Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Crawl New York Times articles for the given query\n",
    "destinationDir = \"../new_data\"\n",
    "nQuery = \"golf\"\n",
    "nytMain(nQuery, destinationDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Crawl Common Crawl Index for the given query\n",
    "#We use usatoday.com as a source for our sports news from USA\n",
    "cQuery = \"golf\"\n",
    "domains = [\"usatoday.com/story/sports/\"+cQuery+\"/2019/01/*\",\"usatoday.com/story/sports/\"+cQuery+\"/2019/02/*\",\n",
    "           \"usatoday.com/story/sports/\"+cQuery+\"/2019/03/*\"]\n",
    "ccMain(cQuery, destinationDir, domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tQuery = \"golf\"\n",
    "twMain(tQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Tweet # 15858\r"
     ]
    }
   ],
   "source": [
    "#Process the crawled data\n",
    "#Twitter\n",
    "csvFile = \"../new_data/tw/tw_fb.csv\"\n",
    "tDest = \"../data/tw/\"\n",
    "processDataMain(csvFile, tDest, \"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing File # 295\r"
     ]
    }
   ],
   "source": [
    "#New York Times or Common Crawl\n",
    "articleDir = \"../new_data/nyt/golf/\"\n",
    "aDest = \"../data/nyt/\"\n",
    "processDataMain(articleDir, aDest, \"nyt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
